{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\torch191\\lib\\site-packages\\pkg_resources\\__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n",
      "  warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n",
      "d:\\miniconda3\\envs\\torch191\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "d:\\miniconda3\\envs\\torch191\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "2025-03-18 09:39:48,971\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "2025-03-18 09:39:50,091\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n",
      "2025-03-18 09:39:50,092\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "2025-03-18 09:39:50,093\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "d:\\miniconda3\\envs\\torch191\\lib\\site-packages\\ray\\_private\\ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
      "  warnings.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m d:\\miniconda3\\envs\\torch191\\lib\\site-packages\\pkg_resources\\__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m   warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m d:\\miniconda3\\envs\\torch191\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m   declare_namespace(pkg)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m d:\\miniconda3\\envs\\torch191\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m   declare_namespace(pkg)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m d:\\miniconda3\\envs\\torch191\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m d:\\miniconda3\\envs\\torch191\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m   deprecation(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=8812)\u001b[0m 2025-03-18 09:39:51,403\tWARNING env.py:154 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "2025-03-18 09:39:51,500\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 100\n",
      "counters:\n",
      "  last_target_update_ts: 100\n",
      "  num_agent_steps_sampled: 100\n",
      "  num_agent_steps_trained: 100\n",
      "  num_env_steps_sampled: 100\n",
      "  num_env_steps_trained: 100\n",
      "  num_target_updates: 100\n",
      "custom_metrics: {}\n",
      "date: 2025-03-18_09-39-52\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 7e19f2cbbaf44dab864898c6bb49c2d1\n",
      "hostname: wei\n",
      "info:\n",
      "  last_target_update_ts: 100\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        actor_loss: -0.49013596773147583\n",
      "        alpha_loss: 0.0\n",
      "        alpha_value:\n",
      "        - 1.0\n",
      "        cql_loss: 20.608327865600586\n",
      "        critic_loss: 62.7724723815918\n",
      "        log_alpha_value:\n",
      "        - 0.0\n",
      "        max_q: 0.0021238019689917564\n",
      "        mean_q: 0.0021238019689917564\n",
      "        min_q: 0.0021238019689917564\n",
      "        policy_t: 0.26833271980285645\n",
      "        target_entropy:\n",
      "        - -1.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 1\n",
      "  num_agent_steps_sampled: 100\n",
      "  num_agent_steps_trained: 100\n",
      "  num_env_steps_sampled: 100\n",
      "  num_env_steps_trained: 100\n",
      "  num_target_updates: 100\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 100\n",
      "num_agent_steps_trained: 100\n",
      "num_env_steps_sampled: 100\n",
      "num_env_steps_sampled_this_iter: 100\n",
      "num_env_steps_trained: 100\n",
      "num_env_steps_trained_this_iter: 100\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_recreated_workers: 0\n",
      "num_steps_trained_this_iter: 100\n",
      "perf:\n",
      "  cpu_util_percent: 5.1\n",
      "  ram_util_percent: 41.7\n",
      "pid: 56756\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  hist_stats:\n",
      "    episode_lengths: []\n",
      "    episode_reward: []\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "time_since_restore: 1.1670892238616943\n",
      "time_this_iter_s: 1.1670892238616943\n",
      "time_total_s: 1.1670892238616943\n",
      "timers:\n",
      "  learn_throughput: 400.197\n",
      "  learn_time_ms: 2.499\n",
      "  sample_time_ms: 3.865\n",
      "  synch_weights_time_ms: 1.004\n",
      "  target_net_update_time_ms: 0.96\n",
      "  training_iteration_time_ms: 8.328\n",
      "timestamp: 1742261992\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 100\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "warmup_time: 1.4162015914916992\n",
      "\n",
      "checkpoint saved at save_model\\checkpoint_000001\n",
      "agent_timesteps_total: 225\n",
      "counters:\n",
      "  last_target_update_ts: 225\n",
      "  num_agent_steps_sampled: 225\n",
      "  num_agent_steps_trained: 225\n",
      "  num_env_steps_sampled: 225\n",
      "  num_env_steps_trained: 225\n",
      "  num_target_updates: 225\n",
      "custom_metrics: {}\n",
      "date: 2025-03-18_09-39-53\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -1477.6892991836805\n",
      "episode_reward_mean: -1477.6892991836805\n",
      "episode_reward_min: -1477.6892991836805\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 1\n",
      "experiment_id: 7e19f2cbbaf44dab864898c6bb49c2d1\n",
      "hostname: wei\n",
      "info:\n",
      "  last_target_update_ts: 225\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        actor_loss: 0.009347915649414062\n",
      "        alpha_loss: 0.0\n",
      "        alpha_value:\n",
      "        - 1.0\n",
      "        cql_loss: 20.386613845825195\n",
      "        critic_loss: 69.66641235351562\n",
      "        log_alpha_value:\n",
      "        - 0.0\n",
      "        max_q: 0.0026292407419532537\n",
      "        mean_q: 0.0026292407419532537\n",
      "        min_q: 0.0026292407419532537\n",
      "        policy_t: -0.9531760811805725\n",
      "        target_entropy:\n",
      "        - -1.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 1\n",
      "  num_agent_steps_sampled: 225\n",
      "  num_agent_steps_trained: 225\n",
      "  num_env_steps_sampled: 225\n",
      "  num_env_steps_trained: 225\n",
      "  num_target_updates: 225\n",
      "iterations_since_restore: 2\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 225\n",
      "num_agent_steps_trained: 225\n",
      "num_env_steps_sampled: 225\n",
      "num_env_steps_sampled_this_iter: 125\n",
      "num_env_steps_trained: 225\n",
      "num_env_steps_trained_this_iter: 125\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_recreated_workers: 0\n",
      "num_steps_trained_this_iter: 125\n",
      "perf:\n",
      "  cpu_util_percent: 0.15000000000000002\n",
      "  ram_util_percent: 41.8\n",
      "pid: 56756\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08173009990590863\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0574019102923638\n",
      "  mean_inference_ms: 0.5685618493409283\n",
      "  mean_raw_obs_processing_ms: 0.3552806060926049\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1477.6892991836805\n",
      "  episode_reward_mean: -1477.6892991836805\n",
      "  episode_reward_min: -1477.6892991836805\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1477.6892991836805\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08173009990590863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0574019102923638\n",
      "    mean_inference_ms: 0.5685618493409283\n",
      "    mean_raw_obs_processing_ms: 0.3552806060926049\n",
      "time_since_restore: 2.1714072227478027\n",
      "time_this_iter_s: 1.0043179988861084\n",
      "time_total_s: 2.1714072227478027\n",
      "timers:\n",
      "  learn_throughput: 416.143\n",
      "  learn_time_ms: 2.403\n",
      "  sample_time_ms: 4.043\n",
      "  synch_weights_time_ms: 1.098\n",
      "  target_net_update_time_ms: 0.649\n",
      "  training_iteration_time_ms: 8.293\n",
      "timestamp: 1742261993\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 225\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "warmup_time: 1.4162015914916992\n",
      "\n",
      "agent_timesteps_total: 353\n",
      "counters:\n",
      "  last_target_update_ts: 353\n",
      "  num_agent_steps_sampled: 353\n",
      "  num_agent_steps_trained: 353\n",
      "  num_env_steps_sampled: 353\n",
      "  num_env_steps_trained: 353\n",
      "  num_target_updates: 353\n",
      "custom_metrics: {}\n",
      "date: 2025-03-18_09-39-54\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -1477.6892991836805\n",
      "episode_reward_mean: -1477.6892991836805\n",
      "episode_reward_min: -1477.6892991836805\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 1\n",
      "experiment_id: 7e19f2cbbaf44dab864898c6bb49c2d1\n",
      "hostname: wei\n",
      "info:\n",
      "  last_target_update_ts: 353\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        actor_loss: -0.1977866291999817\n",
      "        alpha_loss: 0.0\n",
      "        alpha_value:\n",
      "        - 1.0\n",
      "        cql_loss: 20.385860443115234\n",
      "        critic_loss: 77.49874877929688\n",
      "        log_alpha_value:\n",
      "        - 0.0\n",
      "        max_q: 0.002837201114743948\n",
      "        mean_q: 0.002837201114743948\n",
      "        min_q: 0.002837201114743948\n",
      "        policy_t: -0.16023224592208862\n",
      "        target_entropy:\n",
      "        - -1.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 1\n",
      "  num_agent_steps_sampled: 353\n",
      "  num_agent_steps_trained: 353\n",
      "  num_env_steps_sampled: 353\n",
      "  num_env_steps_trained: 353\n",
      "  num_target_updates: 353\n",
      "iterations_since_restore: 3\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 353\n",
      "num_agent_steps_trained: 353\n",
      "num_env_steps_sampled: 353\n",
      "num_env_steps_sampled_this_iter: 128\n",
      "num_env_steps_trained: 353\n",
      "num_env_steps_trained_this_iter: 128\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_recreated_workers: 0\n",
      "num_steps_trained_this_iter: 128\n",
      "perf:\n",
      "  cpu_util_percent: 0.3\n",
      "  ram_util_percent: 41.8\n",
      "pid: 56756\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08173009990590863\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0574019102923638\n",
      "  mean_inference_ms: 0.5685618493409283\n",
      "  mean_raw_obs_processing_ms: 0.3552806060926049\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1477.6892991836805\n",
      "  episode_reward_mean: -1477.6892991836805\n",
      "  episode_reward_min: -1477.6892991836805\n",
      "  episodes_this_iter: 0\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1477.6892991836805\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08173009990590863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0574019102923638\n",
      "    mean_inference_ms: 0.5685618493409283\n",
      "    mean_raw_obs_processing_ms: 0.3552806060926049\n",
      "time_since_restore: 3.1808648109436035\n",
      "time_this_iter_s: 1.0094575881958008\n",
      "time_total_s: 3.1808648109436035\n",
      "timers:\n",
      "  learn_throughput: 392.898\n",
      "  learn_time_ms: 2.545\n",
      "  sample_time_ms: 3.505\n",
      "  synch_weights_time_ms: 0.998\n",
      "  target_net_update_time_ms: 0.601\n",
      "  training_iteration_time_ms: 7.65\n",
      "timestamp: 1742261994\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 353\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "warmup_time: 1.4162015914916992\n",
      "\n",
      "agent_timesteps_total: 482\n",
      "counters:\n",
      "  last_target_update_ts: 482\n",
      "  num_agent_steps_sampled: 482\n",
      "  num_agent_steps_trained: 482\n",
      "  num_env_steps_sampled: 482\n",
      "  num_env_steps_trained: 482\n",
      "  num_target_updates: 482\n",
      "custom_metrics: {}\n",
      "date: 2025-03-18_09-39-55\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -1429.1431222466144\n",
      "episode_reward_mean: -1453.4162107151474\n",
      "episode_reward_min: -1477.6892991836805\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 2\n",
      "experiment_id: 7e19f2cbbaf44dab864898c6bb49c2d1\n",
      "hostname: wei\n",
      "info:\n",
      "  last_target_update_ts: 482\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        actor_loss: -0.24837589263916016\n",
      "        alpha_loss: 0.0\n",
      "        alpha_value:\n",
      "        - 1.0\n",
      "        cql_loss: 20.639257431030273\n",
      "        critic_loss: 107.84722137451172\n",
      "        log_alpha_value:\n",
      "        - 0.0\n",
      "        max_q: 0.004620345309376717\n",
      "        mean_q: 0.004620345309376717\n",
      "        min_q: 0.004620345309376717\n",
      "        policy_t: 0.6130216121673584\n",
      "        target_entropy:\n",
      "        - -1.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 1\n",
      "  num_agent_steps_sampled: 482\n",
      "  num_agent_steps_trained: 482\n",
      "  num_env_steps_sampled: 482\n",
      "  num_env_steps_trained: 482\n",
      "  num_target_updates: 482\n",
      "iterations_since_restore: 4\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 482\n",
      "num_agent_steps_trained: 482\n",
      "num_env_steps_sampled: 482\n",
      "num_env_steps_sampled_this_iter: 129\n",
      "num_env_steps_trained: 482\n",
      "num_env_steps_trained_this_iter: 129\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_recreated_workers: 0\n",
      "num_steps_trained_this_iter: 129\n",
      "perf:\n",
      "  cpu_util_percent: 0.2\n",
      "  ram_util_percent: 41.8\n",
      "pid: 56756\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.07192486991883024\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.05660086510353059\n",
      "  mean_inference_ms: 0.5721637078217787\n",
      "  mean_raw_obs_processing_ms: 0.34766508207186797\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1429.1431222466144\n",
      "  episode_reward_mean: -1453.4162107151474\n",
      "  episode_reward_min: -1477.6892991836805\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1477.6892991836805\n",
      "    - -1429.1431222466144\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07192486991883024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05660086510353059\n",
      "    mean_inference_ms: 0.5721637078217787\n",
      "    mean_raw_obs_processing_ms: 0.34766508207186797\n",
      "time_since_restore: 4.188029766082764\n",
      "time_this_iter_s: 1.0071649551391602\n",
      "time_total_s: 4.188029766082764\n",
      "timers:\n",
      "  learn_throughput: 364.456\n",
      "  learn_time_ms: 2.744\n",
      "  sample_time_ms: 3.503\n",
      "  synch_weights_time_ms: 0.499\n",
      "  target_net_update_time_ms: 0.649\n",
      "  training_iteration_time_ms: 7.395\n",
      "timestamp: 1742261995\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 482\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "warmup_time: 1.4162015914916992\n",
      "\n",
      "agent_timesteps_total: 607\n",
      "counters:\n",
      "  last_target_update_ts: 607\n",
      "  num_agent_steps_sampled: 607\n",
      "  num_agent_steps_trained: 607\n",
      "  num_env_steps_sampled: 607\n",
      "  num_env_steps_trained: 607\n",
      "  num_target_updates: 607\n",
      "custom_metrics: {}\n",
      "date: 2025-03-18_09-39-56\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -1358.0134589850768\n",
      "episode_reward_mean: -1421.6152934717904\n",
      "episode_reward_min: -1477.6892991836805\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 3\n",
      "experiment_id: 7e19f2cbbaf44dab864898c6bb49c2d1\n",
      "hostname: wei\n",
      "info:\n",
      "  last_target_update_ts: 607\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        actor_loss: 0.4613560438156128\n",
      "        alpha_loss: 0.0\n",
      "        alpha_value:\n",
      "        - 1.0\n",
      "        cql_loss: 20.59005355834961\n",
      "        critic_loss: 80.2970962524414\n",
      "        log_alpha_value:\n",
      "        - 0.0\n",
      "        max_q: 0.00022424990311264992\n",
      "        mean_q: 0.00022424990311264992\n",
      "        min_q: 0.00022424990311264992\n",
      "        policy_t: 0.9187560081481934\n",
      "        target_entropy:\n",
      "        - -1.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 1\n",
      "  num_agent_steps_sampled: 607\n",
      "  num_agent_steps_trained: 607\n",
      "  num_env_steps_sampled: 607\n",
      "  num_env_steps_trained: 607\n",
      "  num_target_updates: 607\n",
      "iterations_since_restore: 5\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 607\n",
      "num_agent_steps_trained: 607\n",
      "num_env_steps_sampled: 607\n",
      "num_env_steps_sampled_this_iter: 125\n",
      "num_env_steps_trained: 607\n",
      "num_env_steps_trained_this_iter: 125\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_recreated_workers: 0\n",
      "num_steps_trained_this_iter: 125\n",
      "perf:\n",
      "  cpu_util_percent: 0.7999999999999999\n",
      "  ram_util_percent: 41.8\n",
      "pid: 56756\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06826516304016579\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.05825124048966505\n",
      "  mean_inference_ms: 0.5712010476803582\n",
      "  mean_raw_obs_processing_ms: 0.3494802093798062\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1358.0134589850768\n",
      "  episode_reward_mean: -1421.6152934717904\n",
      "  episode_reward_min: -1477.6892991836805\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1477.6892991836805\n",
      "    - -1429.1431222466144\n",
      "    - -1358.0134589850768\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06826516304016579\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05825124048966505\n",
      "    mean_inference_ms: 0.5712010476803582\n",
      "    mean_raw_obs_processing_ms: 0.3494802093798062\n",
      "time_since_restore: 5.190363168716431\n",
      "time_this_iter_s: 1.002333402633667\n",
      "time_total_s: 5.190363168716431\n",
      "timers:\n",
      "  learn_throughput: 417.348\n",
      "  learn_time_ms: 2.396\n",
      "  sample_time_ms: 3.543\n",
      "  synch_weights_time_ms: 0.961\n",
      "  target_net_update_time_ms: 0.949\n",
      "  training_iteration_time_ms: 7.948\n",
      "timestamp: 1742261996\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 607\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "warmup_time: 1.4162015914916992\n",
      "\n",
      "agent_timesteps_total: 735\n",
      "counters:\n",
      "  last_target_update_ts: 735\n",
      "  num_agent_steps_sampled: 735\n",
      "  num_agent_steps_trained: 735\n",
      "  num_env_steps_sampled: 735\n",
      "  num_env_steps_trained: 735\n",
      "  num_target_updates: 735\n",
      "custom_metrics: {}\n",
      "date: 2025-03-18_09-39-57\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -1358.0134589850768\n",
      "episode_reward_mean: -1421.6152934717904\n",
      "episode_reward_min: -1477.6892991836805\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 7e19f2cbbaf44dab864898c6bb49c2d1\n",
      "hostname: wei\n",
      "info:\n",
      "  last_target_update_ts: 735\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        actor_loss: -0.4437706470489502\n",
      "        alpha_loss: 0.0\n",
      "        alpha_value:\n",
      "        - 1.0\n",
      "        cql_loss: 20.28896713256836\n",
      "        critic_loss: 28.334442138671875\n",
      "        log_alpha_value:\n",
      "        - 0.0\n",
      "        max_q: 0.004320402629673481\n",
      "        mean_q: 0.004320402629673481\n",
      "        min_q: 0.004320402629673481\n",
      "        policy_t: 0.5261127948760986\n",
      "        target_entropy:\n",
      "        - -1.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 1\n",
      "  num_agent_steps_sampled: 735\n",
      "  num_agent_steps_trained: 735\n",
      "  num_env_steps_sampled: 735\n",
      "  num_env_steps_trained: 735\n",
      "  num_target_updates: 735\n",
      "iterations_since_restore: 6\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 735\n",
      "num_agent_steps_trained: 735\n",
      "num_env_steps_sampled: 735\n",
      "num_env_steps_sampled_this_iter: 128\n",
      "num_env_steps_trained: 735\n",
      "num_env_steps_trained_this_iter: 128\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_recreated_workers: 0\n",
      "num_steps_trained_this_iter: 128\n",
      "perf:\n",
      "  cpu_util_percent: 0.3\n",
      "  ram_util_percent: 41.8\n",
      "pid: 56756\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06826516304016579\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.05825124048966505\n",
      "  mean_inference_ms: 0.5712010476803582\n",
      "  mean_raw_obs_processing_ms: 0.3494802093798062\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1358.0134589850768\n",
      "  episode_reward_mean: -1421.6152934717904\n",
      "  episode_reward_min: -1477.6892991836805\n",
      "  episodes_this_iter: 0\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1477.6892991836805\n",
      "    - -1429.1431222466144\n",
      "    - -1358.0134589850768\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06826516304016579\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05825124048966505\n",
      "    mean_inference_ms: 0.5712010476803582\n",
      "    mean_raw_obs_processing_ms: 0.3494802093798062\n",
      "time_since_restore: 6.193968772888184\n",
      "time_this_iter_s: 1.003605604171753\n",
      "time_total_s: 6.193968772888184\n",
      "timers:\n",
      "  learn_throughput: 385.173\n",
      "  learn_time_ms: 2.596\n",
      "  sample_time_ms: 3.643\n",
      "  synch_weights_time_ms: 0.799\n",
      "  target_net_update_time_ms: 0.848\n",
      "  training_iteration_time_ms: 7.886\n",
      "timestamp: 1742261997\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 735\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "warmup_time: 1.4162015914916992\n",
      "\n",
      "agent_timesteps_total: 863\n",
      "counters:\n",
      "  last_target_update_ts: 863\n",
      "  num_agent_steps_sampled: 863\n",
      "  num_agent_steps_trained: 863\n",
      "  num_env_steps_sampled: 863\n",
      "  num_env_steps_trained: 863\n",
      "  num_target_updates: 863\n",
      "custom_metrics: {}\n",
      "date: 2025-03-18_09-39-58\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -1168.6946218422531\n",
      "episode_reward_mean: -1358.3851255644063\n",
      "episode_reward_min: -1477.6892991836805\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 4\n",
      "experiment_id: 7e19f2cbbaf44dab864898c6bb49c2d1\n",
      "hostname: wei\n",
      "info:\n",
      "  last_target_update_ts: 863\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        actor_loss: 0.3352106809616089\n",
      "        alpha_loss: 0.0\n",
      "        alpha_value:\n",
      "        - 1.0\n",
      "        cql_loss: 20.598064422607422\n",
      "        critic_loss: 91.18854522705078\n",
      "        log_alpha_value:\n",
      "        - 0.0\n",
      "        max_q: 0.0048681022599339485\n",
      "        mean_q: 0.0048681022599339485\n",
      "        min_q: 0.0048681022599339485\n",
      "        policy_t: 0.8219119310379028\n",
      "        target_entropy:\n",
      "        - -1.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 1\n",
      "  num_agent_steps_sampled: 863\n",
      "  num_agent_steps_trained: 863\n",
      "  num_env_steps_sampled: 863\n",
      "  num_env_steps_trained: 863\n",
      "  num_target_updates: 863\n",
      "iterations_since_restore: 7\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 863\n",
      "num_agent_steps_trained: 863\n",
      "num_env_steps_sampled: 863\n",
      "num_env_steps_sampled_this_iter: 128\n",
      "num_env_steps_trained: 863\n",
      "num_env_steps_trained_this_iter: 128\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_recreated_workers: 0\n",
      "num_steps_trained_this_iter: 128\n",
      "perf:\n",
      "  cpu_util_percent: 0.30000000000000004\n",
      "  ram_util_percent: 41.8\n",
      "pid: 56756\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06566969345764227\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.05856165188385798\n",
      "  mean_inference_ms: 0.5637160919690205\n",
      "  mean_raw_obs_processing_ms: 0.3554521345428344\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1168.6946218422531\n",
      "  episode_reward_mean: -1358.3851255644063\n",
      "  episode_reward_min: -1477.6892991836805\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths: [200, 200, 200, 200]\n",
      "    episode_reward: [-1477.6892991836805, -1429.1431222466144, -1358.0134589850768,\n",
      "      -1168.6946218422531]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06566969345764227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05856165188385798\n",
      "    mean_inference_ms: 0.5637160919690205\n",
      "    mean_raw_obs_processing_ms: 0.3554521345428344\n",
      "time_since_restore: 7.2002081871032715\n",
      "time_this_iter_s: 1.006239414215088\n",
      "time_total_s: 7.2002081871032715\n",
      "timers:\n",
      "  learn_throughput: 333.984\n",
      "  learn_time_ms: 2.994\n",
      "  sample_time_ms: 4.159\n",
      "  synch_weights_time_ms: 0.749\n",
      "  target_net_update_time_ms: 0.499\n",
      "  training_iteration_time_ms: 8.4\n",
      "timestamp: 1742261998\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 863\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "warmup_time: 1.4162015914916992\n",
      "\n",
      "agent_timesteps_total: 992\n",
      "counters:\n",
      "  last_target_update_ts: 992\n",
      "  num_agent_steps_sampled: 992\n",
      "  num_agent_steps_trained: 992\n",
      "  num_env_steps_sampled: 992\n",
      "  num_env_steps_trained: 992\n",
      "  num_target_updates: 992\n",
      "custom_metrics: {}\n",
      "date: 2025-03-18_09-39-59\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -1168.6946218422531\n",
      "episode_reward_mean: -1358.3851255644063\n",
      "episode_reward_min: -1477.6892991836805\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 4\n",
      "experiment_id: 7e19f2cbbaf44dab864898c6bb49c2d1\n",
      "hostname: wei\n",
      "info:\n",
      "  last_target_update_ts: 992\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        actor_loss: -0.508040726184845\n",
      "        alpha_loss: 0.0\n",
      "        alpha_value:\n",
      "        - 1.0\n",
      "        cql_loss: 20.57034683227539\n",
      "        critic_loss: 33.36277770996094\n",
      "        log_alpha_value:\n",
      "        - 0.0\n",
      "        max_q: 0.004027322866022587\n",
      "        mean_q: 0.004027322866022587\n",
      "        min_q: 0.004027322866022587\n",
      "        policy_t: 0.33823251724243164\n",
      "        target_entropy:\n",
      "        - -1.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 1\n",
      "  num_agent_steps_sampled: 992\n",
      "  num_agent_steps_trained: 992\n",
      "  num_env_steps_sampled: 992\n",
      "  num_env_steps_trained: 992\n",
      "  num_target_updates: 992\n",
      "iterations_since_restore: 8\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 992\n",
      "num_agent_steps_trained: 992\n",
      "num_env_steps_sampled: 992\n",
      "num_env_steps_sampled_this_iter: 129\n",
      "num_env_steps_trained: 992\n",
      "num_env_steps_trained_this_iter: 129\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_recreated_workers: 0\n",
      "num_steps_trained_this_iter: 129\n",
      "perf:\n",
      "  cpu_util_percent: 0.1\n",
      "  ram_util_percent: 41.8\n",
      "pid: 56756\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06566969345764227\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.05856165188385798\n",
      "  mean_inference_ms: 0.5637160919690205\n",
      "  mean_raw_obs_processing_ms: 0.3554521345428344\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1168.6946218422531\n",
      "  episode_reward_mean: -1358.3851255644063\n",
      "  episode_reward_min: -1477.6892991836805\n",
      "  episodes_this_iter: 0\n",
      "  hist_stats:\n",
      "    episode_lengths: [200, 200, 200, 200]\n",
      "    episode_reward: [-1477.6892991836805, -1429.1431222466144, -1358.0134589850768,\n",
      "      -1168.6946218422531]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06566969345764227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05856165188385798\n",
      "    mean_inference_ms: 0.5637160919690205\n",
      "    mean_raw_obs_processing_ms: 0.3554521345428344\n",
      "time_since_restore: 8.210391759872437\n",
      "time_this_iter_s: 1.010183572769165\n",
      "time_total_s: 8.210391759872437\n",
      "timers:\n",
      "  learn_throughput: 395.659\n",
      "  learn_time_ms: 2.527\n",
      "  sample_time_ms: 3.396\n",
      "  synch_weights_time_ms: 0.799\n",
      "  target_net_update_time_ms: 0.848\n",
      "  training_iteration_time_ms: 7.67\n",
      "timestamp: 1742261999\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 992\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "warmup_time: 1.4162015914916992\n",
      "\n",
      "agent_timesteps_total: 1117\n",
      "counters:\n",
      "  last_target_update_ts: 1117\n",
      "  num_agent_steps_sampled: 1117\n",
      "  num_agent_steps_trained: 1117\n",
      "  num_env_steps_sampled: 1117\n",
      "  num_env_steps_trained: 1117\n",
      "  num_target_updates: 1117\n",
      "custom_metrics: {}\n",
      "date: 2025-03-18_09-40-00\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -1168.6946218422531\n",
      "episode_reward_mean: -1326.934700006114\n",
      "episode_reward_min: -1477.6892991836805\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 5\n",
      "experiment_id: 7e19f2cbbaf44dab864898c6bb49c2d1\n",
      "hostname: wei\n",
      "info:\n",
      "  last_target_update_ts: 1117\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        actor_loss: -0.14738619327545166\n",
      "        alpha_loss: 0.0\n",
      "        alpha_value:\n",
      "        - 1.0\n",
      "        cql_loss: 20.332170486450195\n",
      "        critic_loss: 84.78422546386719\n",
      "        log_alpha_value:\n",
      "        - 0.0\n",
      "        max_q: 0.0019104161765426397\n",
      "        mean_q: 0.0019104161765426397\n",
      "        min_q: 0.0019104161765426397\n",
      "        policy_t: 0.7280073165893555\n",
      "        target_entropy:\n",
      "        - -1.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 1\n",
      "  num_agent_steps_sampled: 1117\n",
      "  num_agent_steps_trained: 1117\n",
      "  num_env_steps_sampled: 1117\n",
      "  num_env_steps_trained: 1117\n",
      "  num_target_updates: 1117\n",
      "iterations_since_restore: 9\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 1117\n",
      "num_agent_steps_trained: 1117\n",
      "num_env_steps_sampled: 1117\n",
      "num_env_steps_sampled_this_iter: 125\n",
      "num_env_steps_trained: 1117\n",
      "num_env_steps_trained_this_iter: 125\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_recreated_workers: 0\n",
      "num_steps_trained_this_iter: 125\n",
      "perf:\n",
      "  cpu_util_percent: 0.7\n",
      "  ram_util_percent: 41.8\n",
      "pid: 56756\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06470307802020263\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.05886873141013329\n",
      "  mean_inference_ms: 0.5574558570637327\n",
      "  mean_raw_obs_processing_ms: 0.36002331057579084\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1168.6946218422531\n",
      "  episode_reward_mean: -1326.934700006114\n",
      "  episode_reward_min: -1477.6892991836805\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths: [200, 200, 200, 200, 200]\n",
      "    episode_reward: [-1477.6892991836805, -1429.1431222466144, -1358.0134589850768,\n",
      "      -1168.6946218422531, -1201.1329977729451]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06470307802020263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05886873141013329\n",
      "    mean_inference_ms: 0.5574558570637327\n",
      "    mean_raw_obs_processing_ms: 0.36002331057579084\n",
      "time_since_restore: 9.218900203704834\n",
      "time_this_iter_s: 1.0085084438323975\n",
      "time_total_s: 9.218900203704834\n",
      "timers:\n",
      "  learn_throughput: 378.319\n",
      "  learn_time_ms: 2.643\n",
      "  sample_time_ms: 3.543\n",
      "  synch_weights_time_ms: 1.198\n",
      "  target_net_update_time_ms: 0.75\n",
      "  training_iteration_time_ms: 8.134\n",
      "timestamp: 1742262000\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1117\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "warmup_time: 1.4162015914916992\n",
      "\n",
      "agent_timesteps_total: 1246\n",
      "counters:\n",
      "  last_target_update_ts: 1246\n",
      "  num_agent_steps_sampled: 1246\n",
      "  num_agent_steps_trained: 1246\n",
      "  num_env_steps_sampled: 1246\n",
      "  num_env_steps_trained: 1246\n",
      "  num_target_updates: 1246\n",
      "custom_metrics: {}\n",
      "date: 2025-03-18_09-40-01\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -1168.6946218422531\n",
      "episode_reward_mean: -1360.1884081444443\n",
      "episode_reward_min: -1526.456948836096\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 6\n",
      "experiment_id: 7e19f2cbbaf44dab864898c6bb49c2d1\n",
      "hostname: wei\n",
      "info:\n",
      "  last_target_update_ts: 1246\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        actor_loss: 0.5524108409881592\n",
      "        alpha_loss: 0.0\n",
      "        alpha_value:\n",
      "        - 1.0\n",
      "        cql_loss: 20.265737533569336\n",
      "        critic_loss: 90.4171142578125\n",
      "        log_alpha_value:\n",
      "        - 0.0\n",
      "        max_q: 0.0019765424076467752\n",
      "        mean_q: 0.0019765424076467752\n",
      "        min_q: 0.0019765424076467752\n",
      "        policy_t: 0.9097127914428711\n",
      "        target_entropy:\n",
      "        - -1.0\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 1\n",
      "  num_agent_steps_sampled: 1246\n",
      "  num_agent_steps_trained: 1246\n",
      "  num_env_steps_sampled: 1246\n",
      "  num_env_steps_trained: 1246\n",
      "  num_target_updates: 1246\n",
      "iterations_since_restore: 10\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 1246\n",
      "num_agent_steps_trained: 1246\n",
      "num_env_steps_sampled: 1246\n",
      "num_env_steps_sampled_this_iter: 129\n",
      "num_env_steps_trained: 1246\n",
      "num_env_steps_trained_this_iter: 129\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_recreated_workers: 0\n",
      "num_steps_trained_this_iter: 129\n",
      "perf:\n",
      "  cpu_util_percent: 0.2\n",
      "  ram_util_percent: 41.8\n",
      "pid: 56756\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0642126877582737\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.05857040691373729\n",
      "  mean_inference_ms: 0.5532427838971266\n",
      "  mean_raw_obs_processing_ms: 0.36278600026704616\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1168.6946218422531\n",
      "  episode_reward_mean: -1360.1884081444443\n",
      "  episode_reward_min: -1526.456948836096\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths: [200, 200, 200, 200, 200, 200]\n",
      "    episode_reward: [-1477.6892991836805, -1429.1431222466144, -1358.0134589850768,\n",
      "      -1168.6946218422531, -1201.1329977729451, -1526.456948836096]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0642126877582737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05857040691373729\n",
      "    mean_inference_ms: 0.5532427838971266\n",
      "    mean_raw_obs_processing_ms: 0.36278600026704616\n",
      "time_since_restore: 10.221590757369995\n",
      "time_this_iter_s: 1.0026905536651611\n",
      "time_total_s: 10.221590757369995\n",
      "timers:\n",
      "  learn_throughput: 400.878\n",
      "  learn_time_ms: 2.495\n",
      "  sample_time_ms: 3.743\n",
      "  synch_weights_time_ms: 0.999\n",
      "  target_net_update_time_ms: 0.5\n",
      "  training_iteration_time_ms: 7.736\n",
      "timestamp: 1742262001\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1246\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "warmup_time: 1.4162015914916992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import ray.rllib.algorithms.cql as cql\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# 复制默认配置，并指定离线数据路径\n",
    "config = cql.DEFAULT_CONFIG.copy()\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"framework\"] = \"torch\"\n",
    "# 指定离线数据输入路径，RLlib 会自动加载该目录下的 JSON Lines 文件\n",
    "config[\"output\"] = r\"D:\\Desktop\\CQL\\tmp\\pendulum-out\"  # 输出目录\n",
    "config[\"output_max_file_size\"] = 5000000  # 最大文件大小限制\n",
    "\n",
    "# 注意：离线训练时，算法不会与环境交互采集数据\n",
    "algo = cql.CQL(config=config, env=\"Pendulum-v1\")\n",
    "\n",
    "# 离线训练循环\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        checkpoint = algo.save(\"save_model\")\n",
    "        print(\"checkpoint saved at\", checkpoint)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from ray.rllib.evaluation.sample_batch_builder import SampleBatchBuilder\n",
    "from ray.rllib.offline.json_writer import JsonWriter\n",
    "import json\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# 创建数据采集器\n",
    "batch_builder = SampleBatchBuilder()\n",
    "output_dir = \"D:\\\\Desktop\\\\CQL\\\\jsonwriter\\\\pendulum-out\"\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 创建输出文件\n",
    "output_file = os.path.join(output_dir, \"data.json\")\n",
    "\n",
    "def numpy_to_base64(arr):\n",
    "    \"\"\"将numpy数组转换为base64字符串\"\"\"\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        buf = io.BytesIO()\n",
    "        np.save(buf, arr)\n",
    "        return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
    "    return arr\n",
    "\n",
    "# 采集数据\n",
    "with open(output_file, 'w') as f:\n",
    "    for episode in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            # 构建数据样本，使用base64编码\n",
    "            sample = {\n",
    "                \"type\": \"SampleBatch\",\n",
    "                \"obs\": numpy_to_base64(obs),\n",
    "                \"actions\": action.tolist() if isinstance(action, np.ndarray) else action,\n",
    "                \"rewards\": float(reward),\n",
    "                \"dones\": bool(done),\n",
    "                \"infos\": info,\n",
    "                \"new_obs\": numpy_to_base64(new_obs),\n",
    "                \"t\": 0,\n",
    "                \"eps_id\": episode,\n",
    "                \"agent_index\": 0,\n",
    "                \"weights\": 1.0,\n",
    "                \"action_prob\": 1.0,  # 添加这个字段\n",
    "                \"action_logp\": 0.0,  # 添加这个字段\n",
    "                \"prev_actions\": numpy_to_base64(np.zeros_like(action)),  # 添加这个字段\n",
    "                \"prev_rewards\": 0.0  # 添加这个字段\n",
    "            }\n",
    "            \n",
    "            # 直接写入紧凑格式的JSON\n",
    "            f.write(json.dumps(sample, separators=(',', ':')) + '\\n')\n",
    "            \n",
    "            obs = new_obs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch191",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
