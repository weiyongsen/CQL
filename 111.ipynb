{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 30981)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations[idx]),\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[idx]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_prob\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mFloatTensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_probs[idx]])\n\u001b[0;32m     35\u001b[0m         }\n\u001b[1;32m---> 38\u001b[0m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexam\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata_collect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[1;34m(self, data_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, file), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 14\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 提取需要的字段\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata])\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch191\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch191\\lib\\json\\__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    355\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    356\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\torch191\\lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 30981)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data = []\n",
    "        # 读取json文件\n",
    "        for file in os.listdir(data_path):\n",
    "            if file.endswith('.json'):\n",
    "                with open(os.path.join(data_path, file), 'r') as f:\n",
    "                    self.data.extend(json.load(f))\n",
    "        \n",
    "        # 提取需要的字段\n",
    "        self.observations = np.array([item['obs'] for item in self.data])\n",
    "        self.actions = np.array([item['actions'] for item in self.data])\n",
    "        self.rewards = np.array([item['rewards'] for item in self.data])\n",
    "        self.next_observations = np.array([item['new_obs'] for item in self.data])\n",
    "        self.dones = np.array([item['dones'] for item in self.data])\n",
    "        self.action_probs = np.array([item['action_prob'] for item in self.data])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'obs': torch.FloatTensor(self.observations[idx]),\n",
    "            'action': torch.FloatTensor(self.actions[idx]),\n",
    "            'reward': torch.FloatTensor([self.rewards[idx]]),\n",
    "            'next_obs': torch.FloatTensor(self.next_observations[idx]),\n",
    "            'done': torch.FloatTensor([self.dones[idx]]),\n",
    "            'action_prob': torch.FloatTensor([self.action_probs[idx]])\n",
    "        }\n",
    "    \n",
    "\n",
    "CustomDataset('exam\\data_collect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class CQL:\n",
    "    def __init__(self, state_dim, action_dim, device='cuda'):\n",
    "        self.actor = Actor(state_dim, action_dim).to(device)\n",
    "        self.critic1 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic2 = Critic(state_dim, action_dim).to(device)\n",
    "        \n",
    "        self.actor_optimizer = Adam(self.actor.parameters(), lr=0.0001)\n",
    "        self.critic1_optimizer = Adam(self.critic1.parameters(), lr=0.0003)\n",
    "        self.critic2_optimizer = Adam(self.critic2.parameters(), lr=0.0003)\n",
    "        \n",
    "        self.device = device\n",
    "        self.temperature = 10.0\n",
    "        self.min_q_weight = 0.01\n",
    "        self.num_actions = 10\n",
    "        \n",
    "    def train_step(self, batch):\n",
    "        obs = batch['obs'].to(self.device)\n",
    "        actions = batch['action'].to(self.device)\n",
    "        rewards = batch['reward'].to(self.device)\n",
    "        next_obs = batch['next_obs'].to(self.device)\n",
    "        dones = batch['done'].to(self.device)\n",
    "        action_probs = batch['action_prob'].to(self.device)\n",
    "        \n",
    "        # 计算当前Q值\n",
    "        current_q1 = self.critic1(obs, actions)\n",
    "        current_q2 = self.critic2(obs, actions)\n",
    "        current_q = torch.min(current_q1, current_q2)\n",
    "        \n",
    "        # 采样动作计算Q值\n",
    "        with torch.no_grad():\n",
    "            sampled_actions = torch.randn_like(actions).repeat(self.num_actions, 1)\n",
    "            sampled_obs = obs.repeat(self.num_actions, 1)\n",
    "            sampled_q1 = self.critic1(sampled_obs, sampled_actions)\n",
    "            sampled_q2 = self.critic2(sampled_obs, sampled_actions)\n",
    "            sampled_q = torch.min(sampled_q1, sampled_q2)\n",
    "            sampled_q = sampled_q.view(self.num_actions, -1).mean(0, keepdim=True)\n",
    "        \n",
    "        # 计算保守Q值\n",
    "        conservative_q = current_q - self.temperature * (current_q - sampled_q)\n",
    "        \n",
    "        # 计算目标Q值\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.actor(next_obs)\n",
    "            next_q1 = self.critic1(next_obs, next_actions)\n",
    "            next_q2 = self.critic2(next_obs, next_actions)\n",
    "            next_q = torch.min(next_q1, next_q2)\n",
    "            target_q = rewards + (1 - dones) * 0.99 * next_q\n",
    "        \n",
    "        # 计算损失\n",
    "        critic1_loss = F.mse_loss(current_q1, target_q)\n",
    "        critic2_loss = F.mse_loss(current_q2, target_q)\n",
    "        conservative_loss = F.mse_loss(conservative_q, target_q)\n",
    "        \n",
    "        # 更新critic\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        (critic1_loss + self.min_q_weight * conservative_loss).backward()\n",
    "        self.critic1_optimizer.step()\n",
    "        \n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        (critic2_loss + self.min_q_weight * conservative_loss).backward()\n",
    "        self.critic2_optimizer.step()\n",
    "        \n",
    "        # 更新actor\n",
    "        actor_loss = -self.critic1(obs, self.actor(obs)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'critic1_loss': critic1_loss.item(),\n",
    "            'critic2_loss': critic2_loss.item(),\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'conservative_loss': conservative_loss.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # 加载数据\n",
    "    dataset = CustomDataset(\"collect/sample_save_folder/PPO/2025-04-01_09-45\")\n",
    "    dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    \n",
    "    # 初始化CQL\n",
    "    cql = CQL(state_dim=20, action_dim=4)\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(1000):\n",
    "        epoch_losses = []\n",
    "        for batch in dataloader:\n",
    "            losses = cql.train_step(batch)\n",
    "            epoch_losses.append(losses)\n",
    "        \n",
    "        # 打印训练信息\n",
    "        avg_losses = {k: np.mean([l[k] for l in epoch_losses]) for k in epoch_losses[0].keys()}\n",
    "        print(f\"Epoch {epoch}: {avg_losses}\")\n",
    "        \n",
    "        # 保存模型\n",
    "        if epoch % 50 == 0:\n",
    "            torch.save(cql.actor.state_dict(), f\"save_model/{timestamp}/actor_{epoch}.pth\")\n",
    "            torch.save(cql.critic1.state_dict(), f\"save_model/{timestamp}/critic1_{epoch}.pth\")\n",
    "            torch.save(cql.critic2.state_dict(), f\"save_model/{timestamp}/critic2_{epoch}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch191",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
